# Phase 1.2: Database Setup - Detailed Plan

**Status:** ⏳ **Next - Ready to Start**
**Phase:** Foundation (Week 2)
**Estimated Duration:** 3-4 days
**Dependencies:** Phase 1.1 Development Environment (✅ Complete)
**Target Date:** October 28, 2025

---

## Objective

Initialize TimescaleDB with complete schema, implement hypertables, configure compression and retention policies, and establish a migration framework for future schema changes.

---

## Prerequisites

Before starting this phase, ensure:
- [x] Docker and Docker Compose are installed and running
- [x] Python virtual environment is active
- [x] Development environment from Phase 1.1 is complete
- [ ] PostgreSQL client tools installed (psql)
- [ ] Database migration tool selected (Alembic recommended)

---

## Detailed Tasks

### Task 1: TimescaleDB Docker Setup (Day 1, 2 hours)

**Description:** Set up TimescaleDB container with proper configuration for development and testing.

**Steps:**
1. Update `docker/docker-compose.dev.yml` to include TimescaleDB service
2. Configure TimescaleDB environment variables
3. Set up persistent volume for database data
4. Configure connection pooling settings
5. Add pgAdmin service for database management (dev only)
6. Test container startup and connectivity

**Deliverables:**
- Updated `docker/docker-compose.dev.yml` with TimescaleDB service
- Volume configuration for persistent storage
- Connection test script

**Acceptance Criteria:**
- TimescaleDB container starts without errors
- Can connect to database using psql
- TimescaleDB extension is installed and active
- PgAdmin accessible at http://localhost:5050

**Example Configuration:**
```yaml
services:
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: aetherlens-timescaledb
    environment:
      POSTGRES_DB: aetherlens
      POSTGRES_USER: aetherlens
      POSTGRES_PASSWORD: changeme
      POSTGRES_INITDB_ARGS: "-c shared_preload_libraries=timescaledb"
    ports:
      - "5432:5432"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
      - ./migrations/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U aetherlens"]
      interval: 10s
      timeout: 5s
      retries: 5
```

---

### Task 2: Migration Framework Setup (Day 1, 3 hours)

**Description:** Implement Alembic for database migrations to support schema versioning and evolution.

**Steps:**
1. Install Alembic: `pip install alembic psycopg2-binary`
2. Initialize Alembic in project: `alembic init migrations`
3. Configure `alembic.ini` with database connection
4. Update `env.py` to use AetherLens config
5. Create custom migration template
6. Document migration workflow in DEVELOPMENT.md

**Deliverables:**
```
migrations/
├── alembic.ini                 # Alembic configuration
├── env.py                      # Migration environment
├── script.py.mako              # Migration template
└── versions/                   # Migration scripts
    └── .gitkeep
```

**Code Example (`migrations/env.py`):**
```python
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
from aetherlens.config import settings

# Alembic Config object
config = context.config

# Override sqlalchemy.url from settings
config.set_main_option("sqlalchemy.url", settings.database_url)

# Configure logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Import models for autogenerate support
from aetherlens.models import Base
target_metadata = Base.metadata

def run_migrations_online():
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

run_migrations_online()
```

**Acceptance Criteria:**
- Alembic initializes successfully
- Can generate new migration: `alembic revision -m "test"`
- Can run migrations: `alembic upgrade head`
- Migrations tracked in `alembic_version` table

---

### Task 3: Core Schema Creation (Day 1-2, 4 hours)

**Description:** Create initial database schema including devices, metrics, users, and configuration tables.

**Steps:**
1. Create migration: `alembic revision -m "001_initial_schema"`
2. Define all core tables per SCHEMA.md
3. Add foreign key constraints
4. Create necessary enums and custom types
5. Add table comments for documentation
6. Run migration: `alembic upgrade head`
7. Verify all tables created

**Migration File:** `migrations/versions/001_initial_schema.py`

**Tables to Create:**
1. **devices** - Device registry and configuration
2. **users** - User accounts and preferences
3. **api_tokens** - API authentication tokens
4. **rate_schedules** - Electricity rate configurations
5. **alerts** - Alert rules and configurations
6. **plugins** - Plugin registry and status
7. **schema_versions** - Manual migration tracking (if needed)

**SQL Example (from migration):**
```python
def upgrade():
    # Create devices table
    op.create_table(
        'devices',
        sa.Column('device_id', sa.String(100), primary_key=True),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('type', sa.String(50), nullable=False),
        sa.Column('manufacturer', sa.String(100)),
        sa.Column('model', sa.String(100)),
        sa.Column('location', sa.JSON, nullable=False, server_default='{}'),
        sa.Column('capabilities', sa.ARRAY(sa.String), server_default='{}'),
        sa.Column('configuration', sa.JSON, nullable=False, server_default='{}'),
        sa.Column('metadata', sa.JSON, server_default='{}'),
        sa.Column('status', sa.JSON, server_default='{}'),
        sa.Column('created_at', sa.TIMESTAMP, server_default=sa.func.now()),
        sa.Column('updated_at', sa.TIMESTAMP, server_default=sa.func.now()),
    )

    # Add indexes
    op.create_index('idx_devices_type', 'devices', ['type'])
    op.create_index('idx_devices_location', 'devices', ['location'],
                    postgresql_using='gin')

    # ... repeat for other tables
```

**Acceptance Criteria:**
- All tables created successfully
- Foreign keys validated
- Indexes created on appropriate columns
- Can insert and query sample data
- Table structure matches SCHEMA.md specification

---

### Task 4: Hypertable Implementation (Day 2, 3 hours)

**Description:** Convert metrics table to TimescaleDB hypertable for optimized time-series storage.

**Steps:**
1. Create migration: `alembic revision -m "002_create_hypertables"`
2. Create metrics table with proper time column
3. Convert to hypertable using `create_hypertable()`
4. Configure chunk time interval (7 days recommended)
5. Test hypertable creation
6. Verify chunk creation behavior

**Migration File:** `migrations/versions/002_create_hypertables.py`

**Code Example:**
```python
def upgrade():
    # Create metrics table
    op.create_table(
        'metrics',
        sa.Column('time', sa.TIMESTAMP(timezone=True), nullable=False),
        sa.Column('device_id', sa.String(100), nullable=False),
        sa.Column('metric_type', sa.String(50), nullable=False),
        sa.Column('value', sa.Float, nullable=False),
        sa.Column('unit', sa.String(20), nullable=False),
        sa.Column('tags', sa.JSON, server_default='{}'),
        sa.Column('metadata', sa.JSON, server_default='{}'),
    )

    # Convert to hypertable
    op.execute("""
        SELECT create_hypertable(
            'metrics',
            'time',
            chunk_time_interval => INTERVAL '7 days',
            if_not_exists => TRUE
        );
    """)

    # Add foreign key to devices
    op.create_foreign_key(
        'fk_metrics_device',
        'metrics', 'devices',
        ['device_id'], ['device_id'],
        ondelete='CASCADE'
    )

def downgrade():
    op.drop_table('metrics')
```

**Testing:**
```sql
-- Test hypertable creation
SELECT * FROM timescaledb_information.hypertables
WHERE hypertable_name = 'metrics';

-- Test chunk creation
INSERT INTO metrics (time, device_id, metric_type, value, unit)
VALUES
  (NOW(), 'test-device-01', 'power', 100.0, 'watts'),
  (NOW() - INTERVAL '1 day', 'test-device-01', 'power', 105.0, 'watts'),
  (NOW() - INTERVAL '8 days', 'test-device-01', 'power', 110.0, 'watts');

-- Verify chunks created
SELECT * FROM timescaledb_information.chunks
WHERE hypertable_name = 'metrics';
```

**Acceptance Criteria:**
- Metrics table exists and is a hypertable
- Data inserts span multiple chunks
- Chunk interval is 7 days
- Foreign key to devices works
- Query performance is acceptable (<100ms for recent data)

---

### Task 5: Indexes and Optimization (Day 2, 2 hours)

**Description:** Create indexes for common query patterns and optimize database configuration.

**Steps:**
1. Create migration: `alembic revision -m "003_create_indexes"`
2. Add indexes for time-series queries
3. Create GIN indexes for JSONB columns
4. Add partial indexes for recent data
5. Configure TimescaleDB settings in docker-compose
6. Run ANALYZE on all tables

**Migration File:** `migrations/versions/003_create_indexes.py`

**Indexes to Create:**
```python
def upgrade():
    # Time-series query indexes
    op.create_index(
        'idx_metrics_device_time',
        'metrics',
        ['device_id', sa.text('time DESC')]
    )

    op.create_index(
        'idx_metrics_type_time',
        'metrics',
        ['metric_type', sa.text('time DESC')]
    )

    # JSONB indexes for tag queries
    op.create_index(
        'idx_metrics_tags',
        'metrics',
        ['tags'],
        postgresql_using='gin'
    )

    # Partial index for recent data (last 7 days)
    op.execute("""
        CREATE INDEX idx_metrics_recent
        ON metrics (device_id, time DESC)
        WHERE time > NOW() - INTERVAL '7 days';
    """)

    # Device status queries
    op.execute("""
        CREATE INDEX idx_devices_online
        ON devices ((status->>'online'))
        WHERE status->>'online' = 'true';
    """)
```

**Performance Testing:**
```sql
-- Test query performance
EXPLAIN ANALYZE
SELECT device_id, AVG(value) as avg_power
FROM metrics
WHERE time > NOW() - INTERVAL '24 hours'
  AND metric_type = 'power'
GROUP BY device_id;

-- Should use idx_metrics_type_time or idx_metrics_recent
```

**Acceptance Criteria:**
- All indexes created successfully
- Query plans use appropriate indexes
- Recent data queries (<24h) complete in <50ms
- Historical queries (7 days) complete in <500ms
- Database size is reasonable after indexing

---

### Task 6: Continuous Aggregates (Day 3, 3 hours)

**Description:** Create materialized views for pre-aggregated metrics at hourly and daily intervals.

**Steps:**
1. Create migration: `alembic revision -m "004_continuous_aggregates"`
2. Create hourly aggregate view
3. Create daily aggregate view
4. Set up refresh policies
5. Configure lag times appropriately
6. Test aggregate calculations

**Migration File:** `migrations/versions/004_continuous_aggregates.py`

**Code Example:**
```python
def upgrade():
    # Hourly aggregates
    op.execute("""
        CREATE MATERIALIZED VIEW metrics_hourly
        WITH (timescaledb.continuous) AS
        SELECT
            time_bucket('1 hour', time) AS hour,
            device_id,
            metric_type,
            AVG(value) as avg_value,
            MIN(value) as min_value,
            MAX(value) as max_value,
            COUNT(*) as sample_count,
            SUM(CASE WHEN metric_type = 'energy' THEN value ELSE 0 END) as total_energy,
            percentile_cont(0.95) WITHIN GROUP (ORDER BY value) as p95_value
        FROM metrics
        GROUP BY hour, device_id, metric_type
        WITH NO DATA;
    """)

    # Add refresh policy (refresh every hour, lag 1 hour)
    op.execute("""
        SELECT add_continuous_aggregate_policy('metrics_hourly',
            start_offset => INTERVAL '3 hours',
            end_offset => INTERVAL '1 hour',
            schedule_interval => INTERVAL '1 hour');
    """)

    # Daily aggregates
    op.execute("""
        CREATE MATERIALIZED VIEW metrics_daily
        WITH (timescaledb.continuous) AS
        SELECT
            time_bucket('1 day', time) AS day,
            device_id,
            metric_type,
            AVG(value) as avg_value,
            MIN(value) as min_value,
            MAX(value) as max_value,
            SUM(CASE WHEN metric_type = 'energy' THEN value ELSE 0 END) as total_energy,
            percentile_cont(0.5) WITHIN GROUP (ORDER BY value) as median_value
        FROM metrics
        GROUP BY day, device_id, metric_type
        WITH NO DATA;
    """)

    # Add daily refresh policy
    op.execute("""
        SELECT add_continuous_aggregate_policy('metrics_daily',
            start_offset => INTERVAL '3 days',
            end_offset => INTERVAL '1 day',
            schedule_interval => INTERVAL '1 day');
    """)
```

**Testing:**
```sql
-- Insert test data spanning multiple hours
INSERT INTO metrics (time, device_id, metric_type, value, unit)
SELECT
  NOW() - (n || ' hours')::INTERVAL,
  'test-device-01',
  'power',
  100.0 + random() * 50,
  'watts'
FROM generate_series(1, 48) as n;

-- Force refresh (for testing)
CALL refresh_continuous_aggregate('metrics_hourly', NOW() - INTERVAL '2 days', NOW());

-- Query aggregates
SELECT * FROM metrics_hourly
WHERE device_id = 'test-device-01'
ORDER BY hour DESC
LIMIT 24;
```

**Acceptance Criteria:**
- Hourly and daily aggregate views created
- Refresh policies configured correctly
- Aggregates calculate correctly (spot check values)
- Queries against aggregates are fast (<20ms)
- Aggregate updates happen automatically

---

### Task 7: Compression Policies (Day 3, 2 hours)

**Description:** Configure TimescaleDB compression to reduce storage footprint for older data.

**Steps:**
1. Create migration: `alembic revision -m "005_compression_policies"`
2. Enable compression on metrics hypertable
3. Configure segment_by and order_by columns
4. Set compression policy (compress after 7 days)
5. Test compression behavior
6. Verify query performance on compressed chunks

**Migration File:** `migrations/versions/005_compression_policies.py`

**Code Example:**
```python
def upgrade():
    # Enable compression on metrics table
    op.execute("""
        ALTER TABLE metrics SET (
            timescaledb.compress,
            timescaledb.compress_segmentby = 'device_id, metric_type',
            timescaledb.compress_orderby = 'time DESC'
        );
    """)

    # Add compression policy (compress chunks older than 7 days)
    op.execute("""
        SELECT add_compression_policy('metrics', INTERVAL '7 days');
    """)

def downgrade():
    op.execute("SELECT remove_compression_policy('metrics');")
    op.execute("ALTER TABLE metrics SET (timescaledb.compress = false);")
```

**Testing:**
```sql
-- Check compression settings
SELECT * FROM timescaledb_information.compression_settings
WHERE hypertable_name = 'metrics';

-- Check compression policy
SELECT * FROM timescaledb_information.jobs
WHERE proc_name = 'policy_compression';

-- Manually compress a chunk for testing
SELECT compress_chunk(c.chunk_schema || '.' || c.chunk_name)
FROM timescaledb_information.chunks c
WHERE c.hypertable_name = 'metrics'
  AND c.range_end < NOW() - INTERVAL '7 days'
LIMIT 1;

-- Verify compression
SELECT
  hypertable_name,
  compression_status,
  before_compression_total_bytes,
  after_compression_total_bytes,
  pg_size_pretty(before_compression_total_bytes) as before_size,
  pg_size_pretty(after_compression_total_bytes) as after_size
FROM timescaledb_information.compressed_chunk_stats;
```

**Acceptance Criteria:**
- Compression enabled on metrics table
- Compression policy active (check `timescaledb_information.jobs`)
- Compressed chunks show >70% size reduction
- Queries on compressed data still performant
- Can decompress chunks if needed

---

### Task 8: Retention Policies (Day 3, 2 hours)

**Description:** Set up automatic data retention to prevent infinite storage growth.

**Steps:**
1. Create migration: `alembic revision -m "006_retention_policies"`
2. Add retention policy for raw metrics (90 days)
3. Configure retention for aggregates (1 year for hourly, 5 years for daily)
4. Create retention job schedule
5. Test retention behavior
6. Document retention rules in code comments

**Migration File:** `migrations/versions/006_retention_policies.py`

**Code Example:**
```python
def upgrade():
    # Retention policy for raw metrics (drop chunks older than 90 days)
    op.execute("""
        SELECT add_retention_policy('metrics', INTERVAL '90 days');
    """)

    # Retention for hourly aggregates (1 year)
    op.execute("""
        SELECT add_retention_policy('metrics_hourly', INTERVAL '1 year');
    """)

    # Retention for daily aggregates (5 years)
    op.execute("""
        SELECT add_retention_policy('metrics_daily', INTERVAL '5 years');
    """)

def downgrade():
    op.execute("SELECT remove_retention_policy('metrics');")
    op.execute("SELECT remove_retention_policy('metrics_hourly');")
    op.execute("SELECT remove_retention_policy('metrics_daily');")
```

**Testing:**
```sql
-- Check retention policies
SELECT * FROM timescaledb_information.jobs
WHERE proc_name = 'policy_retention';

-- Simulate old data for testing
INSERT INTO metrics (time, device_id, metric_type, value, unit)
VALUES (NOW() - INTERVAL '100 days', 'old-device', 'power', 100.0, 'watts');

-- Manually run retention job (for testing only)
CALL run_job((SELECT job_id FROM timescaledb_information.jobs
              WHERE proc_name = 'policy_retention' LIMIT 1));

-- Verify old data is deleted
SELECT COUNT(*) FROM metrics WHERE time < NOW() - INTERVAL '90 days';
-- Should return 0
```

**Acceptance Criteria:**
- Retention policies active on all time-series tables
- Old data automatically deleted per schedule
- Retention intervals match requirements (90d, 1y, 5y)
- Jobs run successfully without errors
- Documentation includes retention rules

---

### Task 9: Sample Data and Testing (Day 4, 2 hours)

**Description:** Create sample data for development and testing purposes.

**Steps:**
1. Create `migrations/seeds/sample_data.sql`
2. Add sample devices (3-5 realistic devices)
3. Add sample rate schedules (TOU rates)
4. Generate sample metrics (last 30 days)
5. Create helper script to load sample data
6. Document sample data in README

**Sample Data Script:** `migrations/seeds/sample_data.sql`

```sql
-- Sample devices
INSERT INTO devices (device_id, name, type, manufacturer, model, location, capabilities, configuration) VALUES
('shelly-plug-office-01', 'Office Desk Plug', 'smart_plug', 'Shelly', 'Plug S',
 '{"room": "office", "floor": 2}',
 ARRAY['power_monitoring', 'on_off_control'],
 '{"ip_address": "192.168.1.100", "polling_interval": 30}'),

('tp-link-living-room', 'Living Room Lamp', 'smart_plug', 'TP-Link', 'HS110',
 '{"room": "living_room", "floor": 1}',
 ARRAY['power_monitoring', 'on_off_control'],
 '{"ip_address": "192.168.1.101", "polling_interval": 60}'),

('solar-inverter-01', 'Rooftop Solar Inverter', 'solar_inverter', 'Enphase', 'IQ7+',
 '{"room": "garage", "floor": 1}',
 ARRAY['power_generation', 'energy_metering'],
 '{"ip_address": "192.168.1.102", "polling_interval": 300}');

-- Sample rate schedule
INSERT INTO rate_schedules (rate_id, provider, plan_name, effective_date, currency, time_zone, rate_structure) VALUES
('test-tou-schedule', 'Sample Utility', 'Time-of-Use', '2024-01-01', 'USD', 'America/Los_Angeles',
 '{
   "type": "time_of_use",
   "periods": [
     {"name": "peak", "rate": 0.42, "days": ["weekday"], "hours": "16:00-21:00"},
     {"name": "off_peak", "rate": 0.24, "days": ["all"], "hours": "00:00-15:59,21:00-23:59"}
   ]
 }');

-- Generate sample metrics (last 7 days, 5-minute intervals)
INSERT INTO metrics (time, device_id, metric_type, value, unit)
SELECT
  NOW() - (n || ' minutes')::INTERVAL as time,
  'shelly-plug-office-01' as device_id,
  'power' as metric_type,
  100.0 + (random() * 50) as value,
  'watts' as unit
FROM generate_series(0, 7 * 24 * 60 / 5) as n;

-- Sample user
INSERT INTO users (user_id, username, email, role, preferences) VALUES
('usr-001', 'admin', 'admin@aetherlens.local', 'admin',
 '{"timezone": "America/Los_Angeles", "currency": "USD", "theme": "dark"}');
```

**Helper Script:** `scripts/load_sample_data.sh`
```bash
#!/bin/bash
# Load sample data into development database

DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-aetherlens}
DB_USER=${DB_USER:-aetherlens}

echo "Loading sample data into $DB_NAME..."

psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME \
  -f migrations/seeds/sample_data.sql

echo "Sample data loaded successfully!"
```

**Acceptance Criteria:**
- Sample data script runs without errors
- Can query sample devices and metrics
- Sample data is realistic and useful for development
- Helper script documented in DEVELOPMENT.md
- Sample data covers common use cases

---

### Task 10: Backup and Recovery Scripts (Day 4, 2 hours)

**Description:** Create scripts for database backup and recovery.

**Steps:**
1. Create `scripts/backup_database.sh`
2. Create `scripts/restore_database.sh`
3. Test backup creation
4. Test restore from backup
5. Document backup procedures in DEVELOPMENT.md
6. Add backup to CI/CD if appropriate

**Backup Script:** `scripts/backup_database.sh`
```bash
#!/bin/bash
# Backup AetherLens database

set -e

BACKUP_DIR=${BACKUP_DIR:-./backups}
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME=${DB_NAME:-aetherlens}
DB_USER=${DB_USER:-aetherlens}
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}

mkdir -p $BACKUP_DIR

echo "Creating backup of $DB_NAME..."

# Full backup
pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -F c -b -v \
  -f $BACKUP_DIR/aetherlens_$DATE.backup $DB_NAME

# Plain SQL backup (human-readable)
pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER \
  -f $BACKUP_DIR/aetherlens_$DATE.sql $DB_NAME

echo "Backup created: $BACKUP_DIR/aetherlens_$DATE.backup"

# Keep only last 7 backups
ls -t $BACKUP_DIR/aetherlens_*.backup | tail -n +8 | xargs rm -f
```

**Restore Script:** `scripts/restore_database.sh`
```bash
#!/bin/bash
# Restore AetherLens database from backup

set -e

if [ -z "$1" ]; then
  echo "Usage: $0 <backup_file>"
  exit 1
fi

BACKUP_FILE=$1
DB_NAME=${DB_NAME:-aetherlens}
DB_USER=${DB_USER:-aetherlens}
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}

echo "WARNING: This will drop and recreate the database!"
read -p "Continue? (yes/no) " -r
if [[ ! $REPLY =~ ^yes$ ]]; then
  echo "Aborted."
  exit 1
fi

# Drop existing database
dropdb -h $DB_HOST -p $DB_PORT -U $DB_USER --if-exists $DB_NAME

# Create fresh database
createdb -h $DB_HOST -p $DB_PORT -U $DB_USER $DB_NAME

# Restore from backup
pg_restore -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -v $BACKUP_FILE

echo "Database restored from $BACKUP_FILE"
```

**Acceptance Criteria:**
- Backup script creates valid backup files
- Restore script successfully recreates database
- Backup includes schema and data
- Scripts are documented and easy to use
- Automated backup retention (keep last 7)

---

## Deliverables Summary

Upon completion of Phase 1.2, the following will be delivered:

### 1. Docker Configuration
- [x] `docker/docker-compose.dev.yml` - Updated with TimescaleDB
- [x] TimescaleDB healthcheck configured
- [x] PgAdmin service for database management
- [x] Volume configuration for persistent storage

### 2. Migration Framework
```
migrations/
├── alembic.ini                           # Alembic config
├── env.py                                # Migration environment
├── script.py.mako                        # Migration template
├── versions/                             # Migration scripts
│   ├── 001_initial_schema.py            # Core tables
│   ├── 002_create_hypertables.py        # TimescaleDB hypertables
│   ├── 003_create_indexes.py            # Performance indexes
│   ├── 004_continuous_aggregates.py     # Hourly/daily aggregates
│   ├── 005_compression_policies.py      # Data compression
│   └── 006_retention_policies.py        # Data retention
└── seeds/
    └── sample_data.sql                   # Sample development data
```

### 3. Database Schema
- ✅ All core tables created (devices, users, rate_schedules, alerts, plugins)
- ✅ Metrics hypertable configured
- ✅ Indexes optimized for query patterns
- ✅ Continuous aggregates (hourly, daily)
- ✅ Compression enabled (7-day threshold)
- ✅ Retention policies (90 days raw, 1yr hourly, 5yr daily)

### 4. Scripts and Tools
```
scripts/
├── backup_database.sh        # Database backup script
├── restore_database.sh       # Database restore script
└── load_sample_data.sh       # Load sample data for development
```

### 5. Documentation Updates
- Updated `DEVELOPMENT.md` with database setup instructions
- Migration workflow documented
- Backup and restore procedures documented
- Sample data usage explained

---

## Testing Checklist

### Functional Tests
- [ ] TimescaleDB container starts and stays healthy
- [ ] Can connect to database using psql
- [ ] All migrations run successfully: `alembic upgrade head`
- [ ] Can rollback migrations: `alembic downgrade -1`
- [ ] Sample data loads without errors
- [ ] Hypertables create chunks automatically
- [ ] Continuous aggregates refresh on schedule
- [ ] Compression reduces storage by >70%
- [ ] Retention policies delete old data
- [ ] Backup script creates valid backups
- [ ] Restore script recreates database correctly

### Performance Tests
- [ ] Recent data queries (<24h) complete in <50ms
- [ ] Historical queries (7 days) complete in <500ms
- [ ] Aggregate queries complete in <20ms
- [ ] Bulk insert of 1000 metrics completes in <1s
- [ ] Database size is reasonable (<100MB with sample data)

### Query Tests
```sql
-- Test 1: Insert and retrieve metrics
INSERT INTO metrics (time, device_id, metric_type, value, unit)
VALUES (NOW(), 'test-device', 'power', 100.0, 'watts');

SELECT * FROM metrics WHERE device_id = 'test-device';

-- Test 2: Query with time range
SELECT device_id, AVG(value) as avg_power
FROM metrics
WHERE time > NOW() - INTERVAL '24 hours'
GROUP BY device_id;

-- Test 3: Query hourly aggregates
SELECT hour, device_id, avg_value
FROM metrics_hourly
WHERE device_id = 'shelly-plug-office-01'
  AND hour > NOW() - INTERVAL '7 days'
ORDER BY hour DESC;

-- Test 4: Check compression ratio
SELECT
  pg_size_pretty(before_compression_total_bytes) as before,
  pg_size_pretty(after_compression_total_bytes) as after,
  ROUND(100 - (after_compression_total_bytes::float /
    before_compression_total_bytes::float * 100), 2) as compression_ratio
FROM timescaledb_information.compressed_chunk_stats;

-- Test 5: Verify retention policy
SELECT * FROM timescaledb_information.jobs
WHERE proc_name IN ('policy_compression', 'policy_retention');
```

---

## Common Issues and Solutions

### Issue 1: TimescaleDB Extension Not Found
**Symptoms:** Error: "could not load library timescaledb"

**Solution:**
```bash
# Ensure correct image is used
docker-compose down
docker pull timescale/timescaledb:latest-pg15
docker-compose up -d

# Verify extension
docker-compose exec timescaledb psql -U aetherlens -d aetherlens \
  -c "SELECT * FROM pg_extension WHERE extname = 'timescaledb';"
```

### Issue 2: Migration Fails with Permission Error
**Symptoms:** "permission denied for schema public"

**Solution:**
```sql
-- Grant necessary permissions
GRANT ALL ON SCHEMA public TO aetherlens;
GRANT ALL ON ALL TABLES IN SCHEMA public TO aetherlens;
```

### Issue 3: Chunks Not Compressing
**Symptoms:** Compression policy exists but chunks remain uncompressed

**Solution:**
```sql
-- Manually trigger compression job
CALL run_job((SELECT job_id FROM timescaledb_information.jobs
              WHERE proc_name = 'policy_compression' LIMIT 1));

-- Check job logs
SELECT * FROM timescaledb_information.job_stats
WHERE job_id = (SELECT job_id FROM timescaledb_information.jobs
                WHERE proc_name = 'policy_compression');
```

### Issue 4: Slow Query Performance
**Symptoms:** Queries taking >1 second

**Solution:**
```sql
-- Check if indexes are being used
EXPLAIN ANALYZE
SELECT * FROM metrics
WHERE device_id = 'test' AND time > NOW() - INTERVAL '24 hours';

-- Rebuild statistics
ANALYZE metrics;

-- Ensure query planner uses correct index
SET enable_seqscan = off;  -- For testing only
```

---

## Dependencies and Prerequisites

### Software Requirements
- Docker Desktop 20.10+
- PostgreSQL client tools (psql) 14+
- Python 3.11+ with pip
- Alembic 1.12+
- SQLAlchemy 2.0+

### Python Packages to Install
```bash
pip install alembic psycopg2-binary sqlalchemy[asyncio] asyncpg
```

### Environment Variables
```bash
# Database connection
DATABASE_URL=postgresql://aetherlens:changeme@localhost:5432/aetherlens
DB_HOST=localhost
DB_PORT=5432
DB_NAME=aetherlens
DB_USER=aetherlens
DB_PASSWORD=changeme

# Development settings
AETHERLENS_ENV=development
PYTHONPATH=src/
```

---

## Timeline and Effort Estimates

| Task | Estimated Time | Dependencies |
|------|----------------|--------------|
| 1. TimescaleDB Docker Setup | 2 hours | Phase 1.1 complete |
| 2. Migration Framework Setup | 3 hours | Task 1 complete |
| 3. Core Schema Creation | 4 hours | Task 2 complete |
| 4. Hypertable Implementation | 3 hours | Task 3 complete |
| 5. Indexes and Optimization | 2 hours | Task 4 complete |
| 6. Continuous Aggregates | 3 hours | Task 4 complete |
| 7. Compression Policies | 2 hours | Task 6 complete |
| 8. Retention Policies | 2 hours | Task 6 complete |
| 9. Sample Data and Testing | 2 hours | Tasks 3-8 complete |
| 10. Backup and Recovery Scripts | 2 hours | Task 3 complete |
| **TOTAL** | **25 hours (~3-4 days)** | |

---

## Success Criteria

Phase 1.2 is considered complete when:

### Technical Criteria
- ✅ TimescaleDB running and accessible
- ✅ All migrations run successfully
- ✅ All 10 core tables created
- ✅ Hypertables configured correctly
- ✅ Compression and retention policies active
- ✅ Continuous aggregates refreshing automatically
- ✅ All indexes created and performant
- ✅ Sample data loads successfully
- ✅ Backup and restore scripts working

### Performance Criteria
- ✅ Recent queries (<24h) complete in <50ms
- ✅ Historical queries (7 days) complete in <500ms
- ✅ Bulk inserts (1000 records) complete in <1s
- ✅ Compression achieves >70% reduction
- ✅ Database memory usage <512MB

### Quality Criteria
- ✅ All migrations have tests
- ✅ Schema matches SCHEMA.md specification
- ✅ Documentation is complete and accurate
- ✅ No errors in database logs
- ✅ All acceptance criteria met

---

## Next Steps

After completing Phase 1.2, proceed to:

**Phase 1.3: Core API Framework**
- Build FastAPI application structure
- Implement database connection pooling
- Create CRUD operations for devices
- Add authentication and authorization
- Create health and metrics endpoints

---

## References

- [TimescaleDB Documentation](https://docs.timescale.com/)
- [Alembic Documentation](https://alembic.sqlalchemy.org/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [AetherLens SCHEMA.md](../SCHEMA.md)
- [AetherLens ARCHITECTURE.md](../ARCHITECTURE.md)

---

**Document Version:** 1.0
**Created:** October 24, 2025
**Status:** Ready for Execution
**Estimated Completion:** October 28, 2025
